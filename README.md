# Neuronal-netwrok-from-scratch
In diesem Projekt habe ich im Rahmen eines Universitätsmoduls ein einfaches neuronales Netzwerk von Grund auf selbst implementiert, um das Konzept des Gradient Descent besser zu verstehen und praktisch nachzuvollziehen.

Ziel der Aufgabe war es, ohne den Einsatz von Deep-Learning-Frameworks (wie TensorFlow oder PyTorch) ein vollständig funktionales Netzwerk zu entwickeln. Dabei habe ich:
- eine eigene Forward- und Backward-Pass-Logik implementiert
- den Lernprozess mit Gradient Descent zur Gewichtsaktualisierung genutzt
- verschiedene Kostenfunktionen (z.B. MSE, MAE) getestet
- den Einfluss von Lernraten und Hidden-Layer-Grössen analysiert
- Trainingsverläufe grafisch dargestellt, um die Konvergenz zu untersuchen

Das Projekt diente als praxisnahe Übung, um ein tieferes Verständnis für die mathematischen und algorithmischen Grundlagen von Trainingsprozessen in neuronalen Netzen zu entwickeln.


Wenn du willst, kann ich das auch noch in ein README für dein Repository einbauen oder eine englische Version daraus machen.
